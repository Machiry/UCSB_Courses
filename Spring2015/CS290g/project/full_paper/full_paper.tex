\documentclass[twocolumn]{IEEEtran}
\usepackage{epsfig}
%-----------------------

\begin{document}


\title{CACHEOMS: Practical Exploitation of \textbf{Cache} Side Channel \textbf{O}n \textbf{M}ultiprocessor \textbf{S}ystems}


\author{Aravind Machiry \& Varun Kulkarni Somashekhar
\thanks{Authors are with the
Department of Computer Science,
University of California, Santa Barbara, CA 93106.
E-mail: \texttt{\{you,me\}@cs.ucsb.edu}}
\thanks{This work is supported by Motorola.}
}

\markboth{IEEE Transactions On Computers, Vol. 51, No. 5, May 2002}
{Me and You}

\maketitle

\begin{abstract}
Cache side channel is well known attack on cryptographic implementations\cite{bernstein2005cache}\cite{aciccmez2006trace}\cite{banescu2011cache}. This is primarily based on observation that infrequently used information incurs a large timing penalty, thus revealing some information about the frequency of use of the memory blocks. This attack is straight forward on a single processor system with single level of cache, but current systems (both desktop and mobile) are multiprocessor where each processor has multilevel caches also there are some cache levels which are common to a subset of processors. Moreover, latest ARM processors have TrustZone support\cite{frenzel2010arm} which ensures System-On-Chip (SOC) wide security by avoiding shared cache access. In addition to this, there has been lot of work done both on Hardware and Software side to mitigate cache side channel\cite{180212}\cite{wang2007new}. It is high time we revisit and evaluate cache side channel, its bandwidth and implications\cite{184415}.\\
We intend to discuss the attack and defense strategies involved in a cache side channel, specifically following are our goals:
\begin{itemize}
\item Investigate, understand and report cache side channel with examples. We aim to make the documentation simple enough to be understood by a computer science graduate without much knowledge of cryptography or computer architecture.
\item Research on the recent improvements and state of art on cache side channel and clearly report the findings.
\item Evaluate the applicability of these attacks on well known protocols on current multiprocessor systems, both on x86 and ARM.
\end{itemize}
\end{abstract}

\section {Introduction}

The basic arithmetic operations (i.e. addition, multiplication, and 
inversion) in prime and binary extension fields, $GF(p)$ and $GF(2^n)$, 
have several applications in cryptography, such as decipherment 
operation of RSA algorithm \cite{QC82:Fast}, Diffie-Hellman key exchange 
algorithm \cite{DH76:New}, the Government Digital Signature Standard 
\cite{NIST91:DSS} and also elliptic curve cryptography 
\cite{K87:Elliptic,M93:Elliptic}. Recently, speeding up inversion operation
in both fields has been gaining some attention since inversion is the
most time consuming operation in elliptic curve cryptographic algorithms 
when affine coordinates are selected \cite{K95:The,SOOS95:Fast,%
K99:Fast,SK00:The,H01:Efficient}.

In this paper, we will give and analyze multiplicative inversion algorithms
for $GF(p)$ and $GF(2^n)$ which allow very fast and area-efficient, unified
and scalable hardware
implementations. The algorithms are based on the Montgomery inverse algorithms 
given in \cite{K95:The}. 

\section {Background}

In this section we give a brief overview of the background needed to understand the attack presented in this work. After summarizing the design of cache architecture, a short explaination about different types of cache attacks are provided.

\subsection {Cache Design and Operation}

A cache is a small memory placed between the CPU and RAM to reduce the big latency added by retreival of data. Modern processors usually have more than one level of cache to improve the efficiency of memory accesss.Most modern processes offer 3 levels of cache.with the first level (L1) being the closest to the CPU registers. Generally, L1 and L2 caches are each split into instruction caches and separate data caches. L3 usually stores both instructions and data.The cache size is much smaller than the number of directly addressable bytes in main memory. Hence a mapping strategy needs to be adopted.The cache associativity determines how the main memory blocks map into blocks of the cache.

When a memory reference is made by the CPU, the tag address of the main memory block is compared with all the tags in the corresponding set. If the tag is found then this reference qualifies as a cache hit. Meaning that there is no need to retrieve the data from main memory since it is already located in the cache, and the data can be immediately provided to the CPU. Conversely, if the tag is not found in the corresponding set then the memory reference qualifies as a cache miss.The caching mechanism operation is based on the principals of spatial and temporal locality, which help to minimize the number of
cache misses. Temporal locality states that the same data blocks will likely be requested repeatedly during the execution of a process. Spacial locality states that data blocks from nearby addresses are likely to be subsequently accessed. Even though the number of cache
misses is reduced by these principles, they are not eliminated.

Obviously, data in the cache can be accessed much faster than data present only in memory.This is also true for multi-level caches where data accessed from the L1 cache will experience lower latencies, than data accessed from subsequent cache levels. These time differences are used to decide whether a specific portion of memory resides in the cache - implying that the corresponding data has been accessed recently. This resulting information leakage stemming from microarchitectural time differences when data is retrieved from cache rather than memory forms the basis of cache side channel attacks.

\subsection { Types of Cache Side Channel Attacks }

Cache misses occur at every cache level and they are categorized as:

\begin {itemize}
\item \textbf{cold start misses}, which occur when data is first referenced;
\item \textbf{capacity misses}, which occur if the size of the LUTs Look Up Table used by the cipher is larger than the size of the cache; this type of cache misses do not occur for most cryptographic cipher implementations.
\item \textbf{conflict misses}, which occur when at least two elements of LUTs that map to the same cache block are used.
\end {itemize}

Based on this idea, cache attacks can be classified into three groups:

\begin {itemize}

\item \textbf{Reset attacks} require that all or most LUTs used by the cryptographic cipher are not to be loaded in the cache before the attack commences. Therefore this type of attacks is mainly based on cold start misses.

\item \textbf{Initialization attacks} require the adversary to be able to set the cache into a known state before the attack commences. Therefore this type of attacks is based both on cold start misses and conflict misses.

\item \textbf{Micro-architecture attacks} require the cache to hold all or most LUTs that will be used by the cipher,before the attack commences. Therefore this type of attacks is partially based only on conflict misses and partially on other timing penalties that
strongly depend on the CPU micro-architecture. 

\end {itemize}

These three types of attacks are not limited to timing channels. Measuring power dissipation levels during the encryption process also offers more information about data access. However, this work would focus only on timing attacks.


\end{document}

